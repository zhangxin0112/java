Spark Command: /usr/java/jdk1.8.0_66/bin/java -cp /opt/spark/spark_source_code/spark/conf/:/opt/spark/spark_source_code/spark/assembly/target/scala-2.11/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server spark-internal
========================================
17/11/01 13:23:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/11/01 13:23:27 INFO HiveThriftServer2: Started daemon with process name: 1537@node3
17/11/01 13:23:27 INFO SignalUtils: Registered signal handler for TERM
17/11/01 13:23:27 INFO SignalUtils: Registered signal handler for HUP
17/11/01 13:23:27 INFO SignalUtils: Registered signal handler for INT
17/11/01 13:23:27 INFO HiveThriftServer2: Starting SparkContext
17/11/01 13:23:27 INFO SparkContext: Running Spark version 2.3.0-SNAPSHOT
17/11/01 13:23:27 INFO SparkContext: Submitted application: Thrift JDBC/ODBC Server
17/11/01 13:23:27 INFO SecurityManager: Changing view acls to: root
17/11/01 13:23:27 INFO SecurityManager: Changing modify acls to: root
17/11/01 13:23:27 INFO SecurityManager: Changing view acls groups to: 
17/11/01 13:23:27 INFO SecurityManager: Changing modify acls groups to: 
17/11/01 13:23:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/11/01 13:23:27 INFO Utils: Successfully started service 'sparkDriver' on port 50623.
17/11/01 13:23:27 INFO SparkEnv: Registering MapOutputTracker
17/11/01 13:23:27 INFO SparkEnv: Registering BlockManagerMaster
17/11/01 13:23:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/11/01 13:23:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/11/01 13:23:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-56f258c2-d2bf-4e57-be3a-39da2ea1835b
17/11/01 13:23:28 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/11/01 13:23:28 INFO SparkEnv: Registering OutputCommitCoordinator
17/11/01 13:23:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/11/01 13:23:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://node3:4040
17/11/01 13:23:28 INFO Executor: Starting executor ID driver on host localhost
17/11/01 13:23:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52353.
17/11/01 13:23:28 INFO NettyBlockTransferService: Server created on node3:52353
17/11/01 13:23:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/11/01 13:23:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, node3, 52353, None)
17/11/01 13:23:28 INFO BlockManagerMasterEndpoint: Registering block manager node3:52353 with 366.3 MB RAM, BlockManagerId(driver, node3, 52353, None)
17/11/01 13:23:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, node3, 52353, None)
17/11/01 13:23:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, node3, 52353, None)
17/11/01 13:23:28 INFO SharedState: loading hive config file: file:/opt/spark/spark_source_code/spark/conf/hive-site.xml
17/11/01 13:23:28 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
17/11/01 13:23:28 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
17/11/01 13:23:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/11/01 13:23:29 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:29 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:29 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:23:29 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:23:29 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/11/01 13:23:29 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/11/01 13:23:30 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:30 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/11/01 13:23:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:32 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:32 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:32 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:23:32 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
17/11/01 13:23:32 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:23:33 INFO HiveMetaStore: Added admin role in metastore
17/11/01 13:23:33 INFO HiveMetaStore: Added public role in metastore
17/11/01 13:23:33 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/11/01 13:23:33 INFO HiveMetaStore: 0: get_all_databases
17/11/01 13:23:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/11/01 13:23:33 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/11/01 13:23:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/11/01 13:23:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:35 INFO SessionState: Created local directory: /tmp/hive/java/83e50e62-10c6-4752-b2ad-1ef9706576be_resources
17/11/01 13:23:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/83e50e62-10c6-4752-b2ad-1ef9706576be
17/11/01 13:23:35 INFO SessionState: Created local directory: /tmp/hive/java/root/83e50e62-10c6-4752-b2ad-1ef9706576be
17/11/01 13:23:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/83e50e62-10c6-4752-b2ad-1ef9706576be/_tmp_space.db
17/11/01 13:23:35 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:23:35 INFO HiveMetaStore: 0: get_database: default
17/11/01 13:23:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:23:35 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:35 INFO SessionState: Created local directory: /tmp/hive/java/92726864-6c24-408c-b2b6-0335f0e072e6_resources
17/11/01 13:23:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/92726864-6c24-408c-b2b6-0335f0e072e6
17/11/01 13:23:35 INFO SessionState: Created local directory: /tmp/hive/java/root/92726864-6c24-408c-b2b6-0335f0e072e6
17/11/01 13:23:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/92726864-6c24-408c-b2b6-0335f0e072e6/_tmp_space.db
17/11/01 13:23:35 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:23:36 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
17/11/01 13:23:36 INFO HiveUtils: Initializing execution hive, version 1.2.1
17/11/01 13:23:36 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:36 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:23:36 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:23:37 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/11/01 13:23:37 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/11/01 13:23:38 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:38 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/11/01 13:23:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:23:40 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:23:40 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/11/01 13:23:40 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/11/01 13:23:40 INFO HiveMetaStore: Added admin role in metastore
17/11/01 13:23:40 INFO HiveMetaStore: Added public role in metastore
17/11/01 13:23:40 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/11/01 13:23:40 INFO HiveMetaStore: 0: get_all_databases
17/11/01 13:23:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/11/01 13:23:40 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/11/01 13:23:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/11/01 13:23:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:23:40 INFO SessionState: Created local directory: /tmp/hive/java/9b40341b-7d02-4b35-8114-c7a3feb0767c_resources
17/11/01 13:23:40 INFO SessionState: Created HDFS directory: /tmp/hive/root/9b40341b-7d02-4b35-8114-c7a3feb0767c
17/11/01 13:23:40 INFO SessionState: Created local directory: /tmp/hive/java/root/9b40341b-7d02-4b35-8114-c7a3feb0767c
17/11/01 13:23:40 INFO SessionState: Created HDFS directory: /tmp/hive/root/9b40341b-7d02-4b35-8114-c7a3feb0767c/_tmp_space.db
17/11/01 13:23:40 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:23:40 INFO SessionManager: Operation log root directory is created: /tmp/hive/java/root/operation_logs
17/11/01 13:23:40 INFO AbstractService: HiveServer2: Async execution pool size 100
17/11/01 13:23:40 INFO AbstractService: Service:OperationManager is inited.
17/11/01 13:23:40 INFO AbstractService: Service: SessionManager is inited.
17/11/01 13:23:40 INFO AbstractService: Service: CLIService is inited.
17/11/01 13:23:40 INFO AbstractService: Service:ThriftBinaryCLIService is inited.
17/11/01 13:23:40 INFO AbstractService: Service: HiveServer2 is inited.
17/11/01 13:23:40 INFO AbstractService: Service:OperationManager is started.
17/11/01 13:23:40 INFO AbstractService: Service:SessionManager is started.
17/11/01 13:23:40 INFO AbstractService: Service:CLIService is started.
17/11/01 13:23:40 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:23:40 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:23:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:23:40 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:23:40 INFO HiveMetaStore: 0: get_databases: default
17/11/01 13:23:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
17/11/01 13:23:40 INFO HiveMetaStore: 0: Shutting down the object store...
17/11/01 13:23:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
17/11/01 13:23:40 INFO HiveMetaStore: 0: Metastore shutdown complete.
17/11/01 13:23:40 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
17/11/01 13:23:40 INFO AbstractService: Service:ThriftBinaryCLIService is started.
17/11/01 13:23:40 INFO AbstractService: Service:HiveServer2 is started.
17/11/01 13:23:40 INFO HiveThriftServer2: HiveThriftServer2 started
17/11/01 13:23:40 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
17/11/01 13:23:53 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 13:23:53 INFO SessionState: Created local directory: /tmp/hive/java/c1a7e3de-31b2-47d3-871c-16c7abbe962c_resources
17/11/01 13:23:53 INFO SessionState: Created HDFS directory: /tmp/hive/anonymous/c1a7e3de-31b2-47d3-871c-16c7abbe962c
17/11/01 13:23:53 INFO SessionState: Created local directory: /tmp/hive/java/root/c1a7e3de-31b2-47d3-871c-16c7abbe962c
17/11/01 13:23:53 INFO SessionState: Created HDFS directory: /tmp/hive/anonymous/c1a7e3de-31b2-47d3-871c-16c7abbe962c/_tmp_space.db
17/11/01 13:23:53 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/c1a7e3de-31b2-47d3-871c-16c7abbe962c
17/11/01 13:23:53 INFO HiveMetaStore: 1: get_database: global_temp
17/11/01 13:23:53 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/11/01 13:23:53 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:23:53 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:23:53 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:23:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
17/11/01 13:23:53 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:23:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/11/01 13:23:53 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:23:53 INFO SessionState: Created local directory: /tmp/hive/java/50ba7bf8-f866-419a-b734-1fdacec57cf7_resources
17/11/01 13:23:53 INFO SessionState: Created HDFS directory: /tmp/hive/root/50ba7bf8-f866-419a-b734-1fdacec57cf7
17/11/01 13:23:53 INFO SessionState: Created local directory: /tmp/hive/java/root/50ba7bf8-f866-419a-b734-1fdacec57cf7
17/11/01 13:23:53 INFO SessionState: Created HDFS directory: /tmp/hive/root/50ba7bf8-f866-419a-b734-1fdacec57cf7/_tmp_space.db
17/11/01 13:23:53 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:23:55 INFO HiveMetaStore: 1: get_database: default
17/11/01 13:23:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:09 INFO ThriftCLIService: Session disconnected without closing properly, close it now
17/11/01 13:24:13 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 13:24:13 INFO SessionState: Created local directory: /tmp/hive/java/b0383c13-4b9c-45a0-9a8e-74ec23d4bc4f_resources
17/11/01 13:24:13 INFO SessionState: Created HDFS directory: /tmp/hive/root/b0383c13-4b9c-45a0-9a8e-74ec23d4bc4f
17/11/01 13:24:13 INFO SessionState: Created local directory: /tmp/hive/java/root/b0383c13-4b9c-45a0-9a8e-74ec23d4bc4f
17/11/01 13:24:13 INFO SessionState: Created HDFS directory: /tmp/hive/root/b0383c13-4b9c-45a0-9a8e-74ec23d4bc4f/_tmp_space.db
17/11/01 13:24:13 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/b0383c13-4b9c-45a0-9a8e-74ec23d4bc4f
17/11/01 13:24:13 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:13 INFO SessionState: Created local directory: /tmp/hive/java/3c377ebe-c73c-4742-aede-80378bea28b6_resources
17/11/01 13:24:13 INFO SessionState: Created HDFS directory: /tmp/hive/root/3c377ebe-c73c-4742-aede-80378bea28b6
17/11/01 13:24:13 INFO SessionState: Created local directory: /tmp/hive/java/root/3c377ebe-c73c-4742-aede-80378bea28b6
17/11/01 13:24:13 INFO SessionState: Created HDFS directory: /tmp/hive/root/3c377ebe-c73c-4742-aede-80378bea28b6/_tmp_space.db
17/11/01 13:24:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:24:13 INFO HiveMetaStore: 2: get_database: default
17/11/01 13:24:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:13 INFO HiveMetaStore: 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:24:13 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:24:13 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:24:13 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
17/11/01 13:24:13 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:24:20 INFO SparkExecuteStatementOperation: Running query 'insert overwrite table default.test_e partition(pt="1") select count(1) from default.test_f' with 63ab0e0f-d479-4782-af33-23e5c26296e0
17/11/01 13:24:20 INFO HiveMetaStore: 3: get_database: default
17/11/01 13:24:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:20 INFO HiveMetaStore: 3: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:24:20 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:24:20 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:24:20 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
17/11/01 13:24:20 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:24:20 INFO HiveMetaStore: 3: get_database: default
17/11/01 13:24:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:20 INFO HiveMetaStore: 3: get_table : db=default tbl=test_f
17/11/01 13:24:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:20 INFO HiveMetaStore: 3: get_table : db=default tbl=test_f
17/11/01 13:24:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:20 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:20 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-20_968_656274383320711476-1
17/11/01 13:24:21 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:21 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:21 INFO FileSourceStrategy: Pruning directories with: 
17/11/01 13:24:21 INFO FileSourceStrategy: Post-Scan Filters: 
17/11/01 13:24:21 INFO FileSourceStrategy: Output Data Schema: struct<>
17/11/01 13:24:21 INFO FileSourceScanExec: Pushed Filters: 
17/11/01 13:24:21 INFO HiveMetaStore: 3: get_database: default
17/11/01 13:24:21 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:21 INFO HiveMetaStore: 3: get_table : db=default tbl=test_f
17/11/01 13:24:21 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:21 INFO HiveMetaStore: 3: get_table : db=default tbl=test_f
17/11/01 13:24:21 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:21 INFO HiveMetaStore: 3: get_partitions : db=default tbl=test_f
17/11/01 13:24:21 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test_f	
17/11/01 13:24:21 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:24:21 INFO ContextCleaner: Cleaned accumulator 8
17/11/01 13:24:22 INFO CodeGenerator: Code generated in 253.511211 ms
17/11/01 13:24:22 INFO CodeGenerator: Code generated in 22.84438 ms
17/11/01 13:24:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 398.4 KB, free 365.9 MB)
17/11/01 13:24:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 40.2 KB, free 365.9 MB)
17/11/01 13:24:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on node3:52353 (size: 40.2 KB, free: 366.3 MB)
17/11/01 13:24:22 INFO SparkContext: Created broadcast 0 from run at AccessController.java:0
17/11/01 13:24:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/11/01 13:24:22 INFO SparkContext: Starting job: run at AccessController.java:0
17/11/01 13:24:22 INFO DAGScheduler: Registering RDD 2 (run at AccessController.java:0)
17/11/01 13:24:22 INFO DAGScheduler: Got job 0 (run at AccessController.java:0) with 1 output partitions
17/11/01 13:24:22 INFO DAGScheduler: Final stage: ResultStage 1 (run at AccessController.java:0)
17/11/01 13:24:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/11/01 13:24:22 INFO DAGScheduler: Missing parents: List()
17/11/01 13:24:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at run at AccessController.java:0), which has no missing parents
17/11/01 13:24:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 377.2 KB, free 365.5 MB)
17/11/01 13:24:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 126.1 KB, free 365.4 MB)
17/11/01 13:24:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on node3:52353 (size: 126.1 KB, free: 366.1 MB)
17/11/01 13:24:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1029
17/11/01 13:24:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
17/11/01 13:24:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/11/01 13:24:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7754 bytes)
17/11/01 13:24:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
17/11/01 13:24:23 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/11/01 13:24:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
17/11/01 13:24:23 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:24:23 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72daf1af
17/11/01 13:24:23 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/11/01 13:24:23 INFO ParquetRecordWriterWrapper: initialize serde with table properties.
17/11/01 13:24:23 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-20_968_656274383320711476-1/-ext-10000/_temporary/0/_temporary/attempt_20171101132423_0001_m_000000_0/part-00000-6e45ab3a-5010-416f-97f1-bbcecc356f2a-c000
17/11/01 13:24:23 INFO CodecConfig: Compression set to false
17/11/01 13:24:23 INFO CodecConfig: Compression: UNCOMPRESSED
17/11/01 13:24:23 INFO ParquetOutputFormat: Parquet block size to 134217728
17/11/01 13:24:23 INFO ParquetOutputFormat: Parquet page size to 1048576
17/11/01 13:24:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/11/01 13:24:23 INFO ParquetOutputFormat: Dictionary is on
17/11/01 13:24:23 INFO ParquetOutputFormat: Validation is off
17/11/01 13:24:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/11/01 13:24:23 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2b7b4e2
17/11/01 13:24:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
17/11/01 13:24:23 INFO ColumnChunkPageWriteStore: written 38B for [name] BINARY: 1 values, 11B raw, 11B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17/11/01 13:24:24 INFO FileOutputCommitter: Saved output of task 'attempt_20171101132423_0001_m_000000_0' to hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-20_968_656274383320711476-1/-ext-10000/_temporary/0/task_20171101132423_0001_m_000000
17/11/01 13:24:24 INFO SparkHadoopMapRedUtil: attempt_20171101132423_0001_m_000000_0: Committed
17/11/01 13:24:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2483 bytes result sent to driver
17/11/01 13:24:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1512 ms on localhost (executor driver) (1/1)
17/11/01 13:24:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/11/01 13:24:24 INFO DAGScheduler: ResultStage 1 (run at AccessController.java:0) finished in 1.798 s
17/11/01 13:24:24 INFO DAGScheduler: Job 0 finished: run at AccessController.java:0, took 1.845482 s
17/11/01 13:24:24 INFO FileFormatWriter: Job null committed.
17/11/01 13:24:24 INFO FileFormatWriter: Finished processing stats for job null.
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:24 INFO HiveMetaStore: 3: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:24:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:24:24 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/pt=1
17/11/01 13:24:24 INFO Hive: Renaming src: hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-20_968_656274383320711476-1/-ext-10000/part-00000-6e45ab3a-5010-416f-97f1-bbcecc356f2a-c000, dest: hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-6e45ab3a-5010-416f-97f1-bbcecc356f2a-c000, Status:true
17/11/01 13:24:25 INFO HiveMetaStore: 3: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:24:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:24:25 INFO HiveMetaStore: 3: alter_partition : db=default tbl=test_e
17/11/01 13:24:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_partition : db=default tbl=test_e	
17/11/01 13:24:25 INFO HiveMetaStore: New partition values:[1]
17/11/01 13:24:25 WARN log: Updating partition stats fast for: test_e
17/11/01 13:24:25 WARN log: Updated size to 209
17/11/01 13:24:25 INFO HiveMetaStore: 3: get_table : db=default tbl=test_e
17/11/01 13:24:25 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:25 INFO CodeGenerator: Code generated in 19.512965 ms
17/11/01 13:24:25 INFO DAGScheduler: Asked to cancel job group 63ab0e0f-d479-4782-af33-23e5c26296e0
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 0
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 21
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 19
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 20
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 4
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 14
17/11/01 13:24:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on node3:52353 in memory (size: 126.1 KB, free: 366.3 MB)
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 2
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 15
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 18
17/11/01 13:24:27 INFO ContextCleaner: Cleaned shuffle 0
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 10
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 6
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 17
17/11/01 13:24:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on node3:52353 in memory (size: 40.2 KB, free: 366.3 MB)
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 5
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 13
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 12
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 11
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 9
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 7
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 1
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 3
17/11/01 13:24:27 INFO ContextCleaner: Cleaned accumulator 16
17/11/01 13:24:35 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 13:24:35 INFO SessionState: Created local directory: /tmp/hive/java/ca503988-fa0f-4529-86e5-6c05b1288358_resources
17/11/01 13:24:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/ca503988-fa0f-4529-86e5-6c05b1288358
17/11/01 13:24:35 INFO SessionState: Created local directory: /tmp/hive/java/root/ca503988-fa0f-4529-86e5-6c05b1288358
17/11/01 13:24:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/ca503988-fa0f-4529-86e5-6c05b1288358/_tmp_space.db
17/11/01 13:24:35 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/ca503988-fa0f-4529-86e5-6c05b1288358
17/11/01 13:24:35 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:35 INFO SessionState: Created local directory: /tmp/hive/java/c36580e0-2400-46be-8450-3ab92e9c2844_resources
17/11/01 13:24:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/c36580e0-2400-46be-8450-3ab92e9c2844
17/11/01 13:24:35 INFO SessionState: Created local directory: /tmp/hive/java/root/c36580e0-2400-46be-8450-3ab92e9c2844
17/11/01 13:24:35 INFO SessionState: Created HDFS directory: /tmp/hive/root/c36580e0-2400-46be-8450-3ab92e9c2844/_tmp_space.db
17/11/01 13:24:35 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:24:35 INFO HiveMetaStore: 4: get_database: default
17/11/01 13:24:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:35 INFO HiveMetaStore: 4: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:24:35 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:24:35 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:24:35 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
17/11/01 13:24:35 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:24:38 INFO SparkExecuteStatementOperation: Running query 'insert overwrite table default.test_e partition(pt="1") select count(1) from default.test_f' with d0ce3a85-fd9f-41ca-b65c-537e59d2de67
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_database: default
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:38 INFO HiveMetaStore: 5: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:24:38 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:24:38 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:24:38 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
17/11/01 13:24:38 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_database: default
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_table : db=default tbl=test_f
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_table : db=default tbl=test_f
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:38 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-38_189_5646380994698946629-2
17/11/01 13:24:38 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:38 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:38 INFO FileSourceStrategy: Pruning directories with: 
17/11/01 13:24:38 INFO FileSourceStrategy: Post-Scan Filters: 
17/11/01 13:24:38 INFO FileSourceStrategy: Output Data Schema: struct<>
17/11/01 13:24:38 INFO FileSourceScanExec: Pushed Filters: 
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_database: default
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_table : db=default tbl=test_f
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_table : db=default tbl=test_f
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:24:38 INFO HiveMetaStore: 5: get_partitions : db=default tbl=test_f
17/11/01 13:24:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test_f	
17/11/01 13:24:38 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:24:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 398.4 KB, free 365.9 MB)
17/11/01 13:24:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 40.2 KB, free 365.9 MB)
17/11/01 13:24:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on node3:52353 (size: 40.2 KB, free: 366.3 MB)
17/11/01 13:24:38 INFO SparkContext: Created broadcast 2 from run at AccessController.java:0
17/11/01 13:24:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/11/01 13:24:38 INFO SparkContext: Starting job: run at AccessController.java:0
17/11/01 13:24:38 INFO DAGScheduler: Registering RDD 7 (run at AccessController.java:0)
17/11/01 13:24:38 INFO DAGScheduler: Got job 1 (run at AccessController.java:0) with 1 output partitions
17/11/01 13:24:38 INFO DAGScheduler: Final stage: ResultStage 3 (run at AccessController.java:0)
17/11/01 13:24:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/11/01 13:24:38 INFO DAGScheduler: Missing parents: List()
17/11/01 13:24:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0), which has no missing parents
17/11/01 13:24:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 377.2 KB, free 365.5 MB)
17/11/01 13:24:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 126.1 KB, free 365.4 MB)
17/11/01 13:24:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on node3:52353 (size: 126.1 KB, free: 366.1 MB)
17/11/01 13:24:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1029
17/11/01 13:24:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
17/11/01 13:24:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/11/01 13:24:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7754 bytes)
17/11/01 13:24:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 1)
17/11/01 13:24:39 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/11/01 13:24:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/11/01 13:24:39 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:24:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:24:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ae8a86e
17/11/01 13:24:39 INFO ParquetRecordWriterWrapper: initialize serde with table properties.
17/11/01 13:24:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-38_189_5646380994698946629-2/-ext-10000/_temporary/0/_temporary/attempt_20171101132439_0003_m_000000_0/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000
17/11/01 13:24:39 INFO CodecConfig: Compression set to false
17/11/01 13:24:39 INFO CodecConfig: Compression: UNCOMPRESSED
17/11/01 13:24:39 INFO ParquetOutputFormat: Parquet block size to 134217728
17/11/01 13:24:39 INFO ParquetOutputFormat: Parquet page size to 1048576
17/11/01 13:24:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/11/01 13:24:39 INFO ParquetOutputFormat: Dictionary is on
17/11/01 13:24:39 INFO ParquetOutputFormat: Validation is off
17/11/01 13:24:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/11/01 13:24:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4895c2e9
17/11/01 13:24:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9
17/11/01 13:24:39 INFO ColumnChunkPageWriteStore: written 38B for [name] BINARY: 1 values, 11B raw, 11B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17/11/01 13:24:39 INFO FileOutputCommitter: Saved output of task 'attempt_20171101132439_0003_m_000000_0' to hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-38_189_5646380994698946629-2/-ext-10000/_temporary/0/task_20171101132439_0003_m_000000
17/11/01 13:24:39 INFO SparkHadoopMapRedUtil: attempt_20171101132439_0003_m_000000_0: Committed
17/11/01 13:24:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 2440 bytes result sent to driver
17/11/01 13:24:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 1) in 150 ms on localhost (executor driver) (1/1)
17/11/01 13:24:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/11/01 13:24:39 INFO DAGScheduler: ResultStage 3 (run at AccessController.java:0) finished in 0.495 s
17/11/01 13:24:39 INFO DAGScheduler: Job 1 finished: run at AccessController.java:0, took 0.501768 s
17/11/01 13:24:39 INFO FileFormatWriter: Job null committed.
17/11/01 13:24:39 INFO FileFormatWriter: Finished processing stats for job null.
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_table : db=default tbl=test_e
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:24:39 INFO HiveMetaStore: 5: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:24:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:24:39 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/pt=1
17/11/01 13:24:39 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-38_189_5646380994698946629-2/-ext-10000/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000 to destination hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.loadPartition(HiveExternalCatalog.scala:833)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:216)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3112)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3111)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:71)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:231)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-38_189_5646380994698946629-2/-ext-10000/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000 to destination hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000
	at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2644)
	at org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(Hive.java:2711)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1403)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1324)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadPartition(HiveShim.scala:829)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadPartition$1.apply$mcV$sp(HiveClientImpl.scala:736)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadPartition$1.apply(HiveClientImpl.scala:734)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadPartition$1.apply(HiveClientImpl.scala:734)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:273)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:211)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:210)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:256)
	at org.apache.spark.sql.hive.client.HiveClientImpl.loadPartition(HiveClientImpl.scala:734)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$loadPartition$1.apply$mcV$sp(HiveExternalCatalog.scala:845)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$loadPartition$1.apply(HiveExternalCatalog.scala:833)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$loadPartition$1.apply(HiveExternalCatalog.scala:833)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 26 more
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:798)
	at org.apache.hadoop.hdfs.DFSClient.getEZForPath(DFSClient.java:2966)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1906)
	at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)
	at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1221)
	at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2607)
	... 46 more
17/11/01 13:24:39 ERROR SparkExecuteStatementOperation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_13-24-38_189_5646380994698946629-2/-ext-10000/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000 to destination hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-6a0b8598-f8f5-46be-9c55-c2671322f0c7-c000;
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:268)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/11/01 13:24:39 INFO DAGScheduler: Asked to cancel job group d0ce3a85-fd9f-41ca-b65c-537e59d2de67
17/11/01 13:25:57 INFO ThriftCLIService: Session disconnected without closing properly, close it now
17/11/01 13:26:12 ERROR HiveThriftServer2: RECEIVED SIGNAL TERM
17/11/01 13:26:12 INFO HiveServer2: Shutting down HiveServer2
17/11/01 13:26:12 INFO ThriftCLIService: Thrift server has stopped
17/11/01 13:26:12 INFO AbstractService: Service:ThriftBinaryCLIService is stopped.
17/11/01 13:26:12 INFO AbstractService: Service:OperationManager is stopped.
17/11/01 13:26:12 INFO AbstractService: Service:SessionManager is stopped.
17/11/01 13:26:12 INFO AbstractService: Service:CLIService is stopped.
17/11/01 13:26:12 INFO AbstractService: Service:HiveServer2 is stopped.
17/11/01 13:26:12 INFO SparkUI: Stopped Spark web UI at http://node3:4040
17/11/01 13:26:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/11/01 13:26:12 INFO MemoryStore: MemoryStore cleared
17/11/01 13:26:12 INFO BlockManager: BlockManager stopped
17/11/01 13:26:12 INFO BlockManagerMaster: BlockManagerMaster stopped
17/11/01 13:26:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/11/01 13:26:12 INFO SparkContext: Successfully stopped SparkContext
17/11/01 13:26:12 INFO ShutdownHookManager: Shutdown hook called
17/11/01 13:26:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-38bb2576-beef-4ec4-8336-d93e7dc202d3
17/11/01 13:26:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-89c070d9-881a-440b-8fa7-1daf6c04603c
17/11/01 13:26:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-2556a67b-249e-4137-817f-1fd53c39447a
