Spark Command: /usr/java/jdk1.8.0_66/bin/java -cp /opt/spark/spark-2.3.0-SNAPSHOT-bin-custom-spark/conf/:/opt/spark/spark-2.3.0-SNAPSHOT-bin-custom-spark/jars/*:/etc/hadoop/conf/ -Xmx1g -Dhdp.version=2.6.0-cdh5.7.0 org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server spark-internal
========================================
17/11/01 19:10:51 INFO HiveThriftServer2: Started daemon with process name: 6102@node3
17/11/01 19:10:51 INFO SignalUtils: Registered signal handler for TERM
17/11/01 19:10:51 INFO SignalUtils: Registered signal handler for HUP
17/11/01 19:10:51 INFO SignalUtils: Registered signal handler for INT
17/11/01 19:10:51 INFO HiveThriftServer2: Starting SparkContext
17/11/01 19:10:52 INFO SparkContext: Running Spark version 2.3.0-SNAPSHOT
17/11/01 19:10:52 INFO SparkContext: Submitted application: Thrift JDBC/ODBC Server
17/11/01 19:10:52 INFO SecurityManager: Changing view acls to: root
17/11/01 19:10:52 INFO SecurityManager: Changing modify acls to: root
17/11/01 19:10:52 INFO SecurityManager: Changing view acls groups to: 
17/11/01 19:10:52 INFO SecurityManager: Changing modify acls groups to: 
17/11/01 19:10:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/11/01 19:10:52 INFO Utils: Successfully started service 'sparkDriver' on port 35465.
17/11/01 19:10:52 INFO SparkEnv: Registering MapOutputTracker
17/11/01 19:10:52 INFO SparkEnv: Registering BlockManagerMaster
17/11/01 19:10:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/11/01 19:10:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/11/01 19:10:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-413b27c0-72ab-44d2-ad3a-7b2ad82ccefc
17/11/01 19:10:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/11/01 19:10:52 INFO SparkEnv: Registering OutputCommitCoordinator
17/11/01 19:10:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/11/01 19:10:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://node3:4040
17/11/01 19:10:53 INFO Executor: Starting executor ID driver on host localhost
17/11/01 19:10:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46141.
17/11/01 19:10:53 INFO NettyBlockTransferService: Server created on node3:46141
17/11/01 19:10:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/11/01 19:10:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, node3, 46141, None)
17/11/01 19:10:53 INFO BlockManagerMasterEndpoint: Registering block manager node3:46141 with 366.3 MB RAM, BlockManagerId(driver, node3, 46141, None)
17/11/01 19:10:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, node3, 46141, None)
17/11/01 19:10:53 INFO BlockManager: external shuffle service port = 7337
17/11/01 19:10:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, node3, 46141, None)
17/11/01 19:10:54 INFO EventLoggingListener: Logging events to hdfs://ns/var/sparkonyarnlogs/local-1509534653105.snappy
17/11/01 19:10:54 INFO SharedState: loading hive config file: file:/opt/spark/spark-2.3.0-SNAPSHOT-bin-custom-spark/conf/hive-site.xml
17/11/01 19:10:54 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
17/11/01 19:10:54 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
17/11/01 19:10:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/11/01 19:10:54 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:10:55 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:10:55 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 19:10:55 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:10:55 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/11/01 19:10:55 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/11/01 19:10:56 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:10:56 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/11/01 19:10:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:10:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:10:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:10:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:10:58 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 19:10:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:10:58 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:10:59 INFO HiveMetaStore: Added admin role in metastore
17/11/01 19:10:59 INFO HiveMetaStore: Added public role in metastore
17/11/01 19:10:59 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/11/01 19:10:59 INFO HiveMetaStore: 0: get_all_databases
17/11/01 19:10:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/11/01 19:10:59 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/11/01 19:10:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/11/01 19:10:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:10:59 INFO SessionState: Created local directory: /tmp/hive/java/eef9dabf-b23e-43c7-a1e7-4600beb59c13_resources
17/11/01 19:10:59 INFO SessionState: Created HDFS directory: /tmp/hive/root/eef9dabf-b23e-43c7-a1e7-4600beb59c13
17/11/01 19:10:59 INFO SessionState: Created local directory: /tmp/hive/java/root/eef9dabf-b23e-43c7-a1e7-4600beb59c13
17/11/01 19:10:59 INFO SessionState: Created HDFS directory: /tmp/hive/root/eef9dabf-b23e-43c7-a1e7-4600beb59c13/_tmp_space.db
17/11/01 19:10:59 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 19:10:59 INFO HiveMetaStore: 0: get_database: default
17/11/01 19:10:59 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:10:59 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:00 INFO SessionState: Created local directory: /tmp/hive/java/a872325c-7283-4014-93f1-d761412e5827_resources
17/11/01 19:11:00 INFO SessionState: Created HDFS directory: /tmp/hive/root/a872325c-7283-4014-93f1-d761412e5827
17/11/01 19:11:00 INFO SessionState: Created local directory: /tmp/hive/java/root/a872325c-7283-4014-93f1-d761412e5827
17/11/01 19:11:00 INFO SessionState: Created HDFS directory: /tmp/hive/root/a872325c-7283-4014-93f1-d761412e5827/_tmp_space.db
17/11/01 19:11:00 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 19:11:00 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
17/11/01 19:11:00 INFO HiveUtils: Initializing execution hive, version 1.2.1
17/11/01 19:11:00 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:01 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 19:11:01 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:11:01 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/11/01 19:11:01 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/11/01 19:11:02 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:02 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/11/01 19:11:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:11:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:11:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:11:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:11:04 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:11:04 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:11:04 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/11/01 19:11:04 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/11/01 19:11:04 INFO HiveMetaStore: Added admin role in metastore
17/11/01 19:11:04 INFO HiveMetaStore: Added public role in metastore
17/11/01 19:11:04 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/11/01 19:11:04 INFO HiveMetaStore: 0: get_all_databases
17/11/01 19:11:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/11/01 19:11:04 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/11/01 19:11:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/11/01 19:11:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 19:11:04 INFO SessionState: Created local directory: /tmp/hive/java/95ce93ed-0578-4c24-b93f-17eaf8307cfe_resources
17/11/01 19:11:04 INFO SessionState: Created HDFS directory: /tmp/hive/root/95ce93ed-0578-4c24-b93f-17eaf8307cfe
17/11/01 19:11:04 INFO SessionState: Created local directory: /tmp/hive/java/root/95ce93ed-0578-4c24-b93f-17eaf8307cfe
17/11/01 19:11:04 INFO SessionState: Created HDFS directory: /tmp/hive/root/95ce93ed-0578-4c24-b93f-17eaf8307cfe/_tmp_space.db
17/11/01 19:11:04 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 19:11:04 INFO SessionManager: Operation log root directory is created: /tmp/hive/java/root/operation_logs
17/11/01 19:11:04 INFO AbstractService: HiveServer2: Async execution pool size 100
17/11/01 19:11:04 INFO AbstractService: Service:OperationManager is inited.
17/11/01 19:11:04 INFO AbstractService: Service: SessionManager is inited.
17/11/01 19:11:04 INFO AbstractService: Service: CLIService is inited.
17/11/01 19:11:04 INFO AbstractService: Service:ThriftBinaryCLIService is inited.
17/11/01 19:11:04 INFO AbstractService: Service: HiveServer2 is inited.
17/11/01 19:11:04 INFO AbstractService: Service:OperationManager is started.
17/11/01 19:11:04 INFO AbstractService: Service:SessionManager is started.
17/11/01 19:11:04 INFO AbstractService: Service:CLIService is started.
17/11/01 19:11:04 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:11:04 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 19:11:04 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:11:04 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:11:04 INFO HiveMetaStore: 0: get_databases: default
17/11/01 19:11:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
17/11/01 19:11:04 INFO HiveMetaStore: 0: Shutting down the object store...
17/11/01 19:11:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
17/11/01 19:11:04 INFO HiveMetaStore: 0: Metastore shutdown complete.
17/11/01 19:11:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
17/11/01 19:11:04 INFO AbstractService: Service:ThriftBinaryCLIService is started.
17/11/01 19:11:04 INFO AbstractService: Service:HiveServer2 is started.
17/11/01 19:11:04 INFO HiveThriftServer2: HiveThriftServer2 started
17/11/01 19:11:04 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
17/11/01 19:11:09 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 19:11:09 INFO SessionState: Created local directory: /tmp/hive/java/f30cc3d5-611e-424d-b124-59ebc9173306_resources
17/11/01 19:11:09 INFO SessionState: Created HDFS directory: /tmp/hive/root/f30cc3d5-611e-424d-b124-59ebc9173306
17/11/01 19:11:09 INFO SessionState: Created local directory: /tmp/hive/java/root/f30cc3d5-611e-424d-b124-59ebc9173306
17/11/01 19:11:09 INFO SessionState: Created HDFS directory: /tmp/hive/root/f30cc3d5-611e-424d-b124-59ebc9173306/_tmp_space.db
17/11/01 19:11:09 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/f30cc3d5-611e-424d-b124-59ebc9173306
17/11/01 19:11:09 INFO HiveMetaStore: 1: get_database: global_temp
17/11/01 19:11:09 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/11/01 19:11:09 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 19:11:09 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:11:09 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 19:11:09 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:11:09 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:11:09 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/11/01 19:11:09 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:09 INFO SessionState: Created local directory: /tmp/hive/java/f1bd5dbb-b025-40dd-b686-8dd2995f0484_resources
17/11/01 19:11:09 INFO SessionState: Created HDFS directory: /tmp/hive/root/f1bd5dbb-b025-40dd-b686-8dd2995f0484
17/11/01 19:11:09 INFO SessionState: Created local directory: /tmp/hive/java/root/f1bd5dbb-b025-40dd-b686-8dd2995f0484
17/11/01 19:11:09 INFO SessionState: Created HDFS directory: /tmp/hive/root/f1bd5dbb-b025-40dd-b686-8dd2995f0484/_tmp_space.db
17/11/01 19:11:09 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 19:11:12 INFO HiveMetaStore: 1: get_database: default
17/11/01 19:11:12 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:14 INFO SparkExecuteStatementOperation: Running query 'insert overwrite table default.test_e partition(pt="1") select count(1) from default.test_f' with eb1a16ac-e10e-48df-a4b1-d7738fa4e25b
17/11/01 19:11:14 INFO HiveMetaStore: 2: get_database: default
17/11/01 19:11:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:14 INFO HiveMetaStore: 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 19:11:14 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:11:14 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 19:11:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:11:14 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:11:14 INFO HiveMetaStore: 2: get_database: default
17/11/01 19:11:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:14 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 19:11:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:15 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 19:11:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:15 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:16 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-16_072_2707658365036635258-1
17/11/01 19:11:16 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:16 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:16 INFO FileSourceStrategy: Pruning directories with: 
17/11/01 19:11:16 INFO FileSourceStrategy: Post-Scan Filters: 
17/11/01 19:11:16 INFO FileSourceStrategy: Output Data Schema: struct<>
17/11/01 19:11:16 INFO FileSourceScanExec: Pushed Filters: 
17/11/01 19:11:16 INFO HiveMetaStore: 2: get_database: default
17/11/01 19:11:16 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:16 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 19:11:16 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:16 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 19:11:16 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:16 INFO HiveMetaStore: 2: get_partitions : db=default tbl=test_f
17/11/01 19:11:16 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test_f	
17/11/01 19:11:16 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 19:11:17 INFO CodeGenerator: Code generated in 394.247018 ms
17/11/01 19:11:17 INFO CodeGenerator: Code generated in 23.132886 ms
17/11/01 19:11:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 412.9 KB, free 365.9 MB)
17/11/01 19:11:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.0 KB, free 365.9 MB)
17/11/01 19:11:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on node3:46141 (size: 42.0 KB, free: 366.3 MB)
17/11/01 19:11:17 INFO SparkContext: Created broadcast 0 from run at AccessController.java:0
17/11/01 19:11:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/11/01 19:11:17 INFO SparkContext: Starting job: run at AccessController.java:0
17/11/01 19:11:17 INFO DAGScheduler: Registering RDD 2 (run at AccessController.java:0)
17/11/01 19:11:17 INFO DAGScheduler: Got job 0 (run at AccessController.java:0) with 1 output partitions
17/11/01 19:11:17 INFO DAGScheduler: Final stage: ResultStage 1 (run at AccessController.java:0)
17/11/01 19:11:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/11/01 19:11:17 INFO DAGScheduler: Missing parents: List()
17/11/01 19:11:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at run at AccessController.java:0), which has no missing parents
17/11/01 19:11:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 404.8 KB, free 365.5 MB)
17/11/01 19:11:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 131.3 KB, free 365.3 MB)
17/11/01 19:11:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on node3:46141 (size: 131.3 KB, free: 366.1 MB)
17/11/01 19:11:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1029
17/11/01 19:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
17/11/01 19:11:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/11/01 19:11:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7754 bytes)
17/11/01 19:11:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
17/11/01 19:11:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/11/01 19:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
17/11/01 19:11:18 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 19:11:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65aa54d
17/11/01 19:11:18 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/11/01 19:11:18 INFO ParquetRecordWriterWrapper: initialize serde with table properties.
17/11/01 19:11:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-16_072_2707658365036635258-1/-ext-10000/_temporary/0/_temporary/attempt_20171101191118_0001_m_000000_0/part-00000-223fa278-f0b6-453b-987c-2f7fa7a12bb1-c000
17/11/01 19:11:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@411ebece
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
17/11/01 19:11:19 INFO ContextCleaner: Cleaned accumulator 8
17/11/01 19:11:19 INFO FileOutputCommitter: Saved output of task 'attempt_20171101191118_0001_m_000000_0' to hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-16_072_2707658365036635258-1/-ext-10000/_temporary/0/task_20171101191118_0001_m_000000
17/11/01 19:11:19 INFO SparkHadoopMapRedUtil: attempt_20171101191118_0001_m_000000_0: Committed
17/11/01 19:11:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 1936 bytes result sent to driver
17/11/01 19:11:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1198 ms on localhost (executor driver) (1/1)
17/11/01 19:11:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/11/01 19:11:19 INFO DAGScheduler: ResultStage 1 (run at AccessController.java:0) finished in 1.740 s
17/11/01 19:11:19 INFO DAGScheduler: Job 0 finished: run at AccessController.java:0, took 1.823662 s
17/11/01 19:11:19 INFO FileFormatWriter: Job null committed.
17/11/01 19:11:19 INFO FileFormatWriter: Finished processing stats for job null.
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:19 INFO HiveMetaStore: 2: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 19:11:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 19:11:19 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/pt=1
17/11/01 19:11:20 INFO Hive: Renaming src: hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-16_072_2707658365036635258-1/-ext-10000/part-00000-223fa278-f0b6-453b-987c-2f7fa7a12bb1-c000, dest: hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-223fa278-f0b6-453b-987c-2f7fa7a12bb1-c000, Status:true
17/11/01 19:11:20 INFO HiveMetaStore: 2: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 19:11:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 19:11:20 INFO HiveMetaStore: 2: alter_partition : db=default tbl=test_e
17/11/01 19:11:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_partition : db=default tbl=test_e	
17/11/01 19:11:20 INFO HiveMetaStore: New partition values:[1]
17/11/01 19:11:20 WARN log: Updating partition stats fast for: test_e
17/11/01 19:11:20 WARN log: Updated size to 209
17/11/01 19:11:20 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 19:11:20 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:20 INFO CodeGenerator: Code generated in 20.947502 ms
17/11/01 19:11:20 INFO DAGScheduler: Asked to cancel job group eb1a16ac-e10e-48df-a4b1-d7738fa4e25b
17/11/01 19:11:24 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 19:11:24 INFO SessionState: Created local directory: /tmp/hive/java/7ae9c7aa-2036-4f0a-933e-5b132b62d913_resources
17/11/01 19:11:24 INFO SessionState: Created HDFS directory: /tmp/hive/root/7ae9c7aa-2036-4f0a-933e-5b132b62d913
17/11/01 19:11:24 INFO SessionState: Created local directory: /tmp/hive/java/root/7ae9c7aa-2036-4f0a-933e-5b132b62d913
17/11/01 19:11:24 INFO SessionState: Created HDFS directory: /tmp/hive/root/7ae9c7aa-2036-4f0a-933e-5b132b62d913/_tmp_space.db
17/11/01 19:11:24 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/7ae9c7aa-2036-4f0a-933e-5b132b62d913
17/11/01 19:11:24 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:24 INFO SessionState: Created local directory: /tmp/hive/java/6942c521-e28f-42ae-887a-46ea7938b7cd_resources
17/11/01 19:11:24 INFO SessionState: Created HDFS directory: /tmp/hive/root/6942c521-e28f-42ae-887a-46ea7938b7cd
17/11/01 19:11:24 INFO SessionState: Created local directory: /tmp/hive/java/root/6942c521-e28f-42ae-887a-46ea7938b7cd
17/11/01 19:11:24 INFO SessionState: Created HDFS directory: /tmp/hive/root/6942c521-e28f-42ae-887a-46ea7938b7cd/_tmp_space.db
17/11/01 19:11:24 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 19:11:24 INFO HiveMetaStore: 3: get_database: default
17/11/01 19:11:24 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:24 INFO HiveMetaStore: 3: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 19:11:24 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:11:24 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 19:11:24 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:11:24 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:11:26 INFO SparkExecuteStatementOperation: Running query 'insert overwrite table default.test_e partition(pt="1") select count(1) from default.test_f' with d23f4882-a69f-41f8-9848-5e8a88ffa528
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_database: default
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:26 INFO HiveMetaStore: 4: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 19:11:26 INFO ObjectStore: ObjectStore, initialize called
17/11/01 19:11:26 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 19:11:26 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 19:11:26 INFO ObjectStore: Initialized ObjectStore
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_database: default
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:26 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-26_460_4331655156540209066-2
17/11/01 19:11:26 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:26 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:26 INFO FileSourceStrategy: Pruning directories with: 
17/11/01 19:11:26 INFO FileSourceStrategy: Post-Scan Filters: 
17/11/01 19:11:26 INFO FileSourceStrategy: Output Data Schema: struct<>
17/11/01 19:11:26 INFO FileSourceScanExec: Pushed Filters: 
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_database: default
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 19:11:26 INFO HiveMetaStore: 4: get_partitions : db=default tbl=test_f
17/11/01 19:11:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test_f	
17/11/01 19:11:26 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 19:11:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 412.9 KB, free 364.9 MB)
17/11/01 19:11:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.0 KB, free 364.9 MB)
17/11/01 19:11:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on node3:46141 (size: 42.0 KB, free: 366.1 MB)
17/11/01 19:11:26 INFO SparkContext: Created broadcast 2 from run at AccessController.java:0
17/11/01 19:11:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/11/01 19:11:27 INFO SparkContext: Starting job: run at AccessController.java:0
17/11/01 19:11:27 INFO DAGScheduler: Registering RDD 7 (run at AccessController.java:0)
17/11/01 19:11:27 INFO DAGScheduler: Got job 1 (run at AccessController.java:0) with 1 output partitions
17/11/01 19:11:27 INFO DAGScheduler: Final stage: ResultStage 3 (run at AccessController.java:0)
17/11/01 19:11:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/11/01 19:11:27 INFO DAGScheduler: Missing parents: List()
17/11/01 19:11:27 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0), which has no missing parents
17/11/01 19:11:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 404.8 KB, free 364.5 MB)
17/11/01 19:11:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 131.3 KB, free 364.4 MB)
17/11/01 19:11:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on node3:46141 (size: 131.3 KB, free: 366.0 MB)
17/11/01 19:11:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1029
17/11/01 19:11:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
17/11/01 19:11:27 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/11/01 19:11:27 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7754 bytes)
17/11/01 19:11:27 INFO Executor: Running task 0.0 in stage 3.0 (TID 1)
17/11/01 19:11:27 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/11/01 19:11:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/11/01 19:11:27 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 19:11:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 19:11:27 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1064da05
17/11/01 19:11:27 INFO ParquetRecordWriterWrapper: initialize serde with table properties.
17/11/01 19:11:27 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-26_460_4331655156540209066-2/-ext-10000/_temporary/0/_temporary/attempt_20171101191127_0003_m_000000_0/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000
17/11/01 19:11:27 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14066e42
17/11/01 19:11:27 INFO FileOutputCommitter: Saved output of task 'attempt_20171101191127_0003_m_000000_0' to hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-26_460_4331655156540209066-2/-ext-10000/_temporary/0/task_20171101191127_0003_m_000000
17/11/01 19:11:27 INFO SparkHadoopMapRedUtil: attempt_20171101191127_0003_m_000000_0: Committed
17/11/01 19:11:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 1893 bytes result sent to driver
17/11/01 19:11:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 1) in 174 ms on localhost (executor driver) (1/1)
17/11/01 19:11:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/11/01 19:11:27 INFO DAGScheduler: ResultStage 3 (run at AccessController.java:0) finished in 0.565 s
17/11/01 19:11:27 INFO DAGScheduler: Job 1 finished: run at AccessController.java:0, took 0.571770 s
17/11/01 19:11:27 INFO FileFormatWriter: Job null committed.
17/11/01 19:11:27 INFO FileFormatWriter: Finished processing stats for job null.
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 19:11:27 INFO HiveMetaStore: 4: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 19:11:27 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 19:11:27 INFO FileUtils: Creating directory if it doesn't exist: hdfs://ns/user/hive/warehouse/test_e/pt=1
17/11/01 19:11:28 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-26_460_4331655156540209066-2/-ext-10000/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000 to destination hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.loadPartition(HiveExternalCatalog.scala:833)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:216)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$49.apply(Dataset.scala:3112)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3111)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:71)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:231)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-26_460_4331655156540209066-2/-ext-10000/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000 to destination hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000
	at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2644)
	at org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(Hive.java:2711)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1403)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1324)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadPartition(HiveShim.scala:829)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadPartition$1.apply$mcV$sp(HiveClientImpl.scala:736)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadPartition$1.apply(HiveClientImpl.scala:734)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadPartition$1.apply(HiveClientImpl.scala:734)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:273)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:211)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:210)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:256)
	at org.apache.spark.sql.hive.client.HiveClientImpl.loadPartition(HiveClientImpl.scala:734)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$loadPartition$1.apply$mcV$sp(HiveExternalCatalog.scala:845)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$loadPartition$1.apply(HiveExternalCatalog.scala:833)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$loadPartition$1.apply(HiveExternalCatalog.scala:833)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 26 more
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:798)
	at org.apache.hadoop.hdfs.DFSClient.getEZForPath(DFSClient.java:2966)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1906)
	at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)
	at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1221)
	at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2607)
	... 46 more
17/11/01 19:11:28 ERROR SparkExecuteStatementOperation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://ns/user/hive/warehouse/test_e/.hive-staging_hive_2017-11-01_19-11-26_460_4331655156540209066-2/-ext-10000/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000 to destination hdfs://ns/user/hive/warehouse/test_e/pt=1/part-00000-d7933dde-5c75-465a-83ed-0e0d034371ee-c000;
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:268)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/11/01 19:11:28 INFO DAGScheduler: Asked to cancel job group d23f4882-a69f-41f8-9848-5e8a88ffa528
17/11/01 19:11:37 ERROR HiveThriftServer2: RECEIVED SIGNAL TERM
17/11/01 19:11:37 INFO HiveServer2: Shutting down HiveServer2
17/11/01 19:11:37 INFO ThriftCLIService: Thrift server has stopped
17/11/01 19:11:37 INFO AbstractService: Service:ThriftBinaryCLIService is stopped.
17/11/01 19:11:37 INFO AbstractService: Service:OperationManager is stopped.
17/11/01 19:11:37 INFO AbstractService: Service:SessionManager is stopped.
17/11/01 19:11:37 INFO AbstractService: Service:CLIService is stopped.
17/11/01 19:11:37 INFO AbstractService: Service:HiveServer2 is stopped.
17/11/01 19:11:37 INFO SparkUI: Stopped Spark web UI at http://node3:4040
17/11/01 19:11:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/11/01 19:11:37 INFO MemoryStore: MemoryStore cleared
17/11/01 19:11:37 INFO BlockManager: BlockManager stopped
17/11/01 19:11:37 INFO BlockManagerMaster: BlockManagerMaster stopped
17/11/01 19:11:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/11/01 19:11:37 INFO SparkContext: Successfully stopped SparkContext
17/11/01 19:11:37 INFO ShutdownHookManager: Shutdown hook called
17/11/01 19:11:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3d6664f-407c-4a49-85d8-e3423ed58619
17/11/01 19:11:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d80f2fbd-3c40-41bf-9bc6-8badf8456a59
17/11/01 19:11:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-8cbfb4ee-b5f2-47c6-ac97-1664def93b8f
