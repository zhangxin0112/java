Spark Command: /usr/java/jdk1.8.0_66/bin/java -cp /opt/spark/spark_source_code/spark/conf/:/opt/spark/spark_source_code/spark/assembly/target/scala-2.11/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server spark-internal
========================================
17/11/01 13:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/11/01 13:26:55 INFO HiveThriftServer2: Started daemon with process name: 2036@node3
17/11/01 13:26:55 INFO SignalUtils: Registered signal handler for TERM
17/11/01 13:26:55 INFO SignalUtils: Registered signal handler for HUP
17/11/01 13:26:55 INFO SignalUtils: Registered signal handler for INT
17/11/01 13:26:55 INFO HiveThriftServer2: Starting SparkContext
17/11/01 13:26:56 INFO SparkContext: Running Spark version 2.3.0-SNAPSHOT
17/11/01 13:26:56 INFO SparkContext: Submitted application: Thrift JDBC/ODBC Server
17/11/01 13:26:56 INFO SecurityManager: Changing view acls to: root
17/11/01 13:26:56 INFO SecurityManager: Changing modify acls to: root
17/11/01 13:26:56 INFO SecurityManager: Changing view acls groups to: 
17/11/01 13:26:56 INFO SecurityManager: Changing modify acls groups to: 
17/11/01 13:26:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
17/11/01 13:26:56 INFO Utils: Successfully started service 'sparkDriver' on port 57102.
17/11/01 13:26:56 INFO SparkEnv: Registering MapOutputTracker
17/11/01 13:26:56 INFO SparkEnv: Registering BlockManagerMaster
17/11/01 13:26:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/11/01 13:26:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/11/01 13:26:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b7baea98-d4f0-408e-b127-a255980a004e
17/11/01 13:26:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/11/01 13:26:56 INFO SparkEnv: Registering OutputCommitCoordinator
17/11/01 13:26:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/11/01 13:26:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://node3:4040
17/11/01 13:26:56 INFO Executor: Starting executor ID driver on host localhost
17/11/01 13:26:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41106.
17/11/01 13:26:56 INFO NettyBlockTransferService: Server created on node3:41106
17/11/01 13:26:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/11/01 13:26:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, node3, 41106, None)
17/11/01 13:26:56 INFO BlockManagerMasterEndpoint: Registering block manager node3:41106 with 366.3 MB RAM, BlockManagerId(driver, node3, 41106, None)
17/11/01 13:26:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, node3, 41106, None)
17/11/01 13:26:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, node3, 41106, None)
17/11/01 13:26:57 INFO SharedState: loading hive config file: file:/opt/spark/spark_source_code/spark/conf/hive-site.xml
17/11/01 13:26:57 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
17/11/01 13:26:57 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
17/11/01 13:26:57 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/11/01 13:26:57 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:26:58 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:26:58 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:26:58 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:26:58 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/11/01 13:26:58 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/11/01 13:26:59 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:26:59 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/11/01 13:27:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:01 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:27:01 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:01 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:02 INFO HiveMetaStore: Added admin role in metastore
17/11/01 13:27:02 INFO HiveMetaStore: Added public role in metastore
17/11/01 13:27:02 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/11/01 13:27:02 INFO HiveMetaStore: 0: get_all_databases
17/11/01 13:27:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/11/01 13:27:02 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/11/01 13:27:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/11/01 13:27:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:03 INFO SessionState: Created local directory: /tmp/hive/java/7edae16c-7198-4e06-be9a-ce384d0cc31d_resources
17/11/01 13:27:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/7edae16c-7198-4e06-be9a-ce384d0cc31d
17/11/01 13:27:03 INFO SessionState: Created local directory: /tmp/hive/java/root/7edae16c-7198-4e06-be9a-ce384d0cc31d
17/11/01 13:27:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/7edae16c-7198-4e06-be9a-ce384d0cc31d/_tmp_space.db
17/11/01 13:27:03 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:27:03 INFO HiveMetaStore: 0: get_database: default
17/11/01 13:27:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:03 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:03 INFO SessionState: Created local directory: /tmp/hive/java/3116a097-bf1a-433e-a74a-eac8a4c281b4_resources
17/11/01 13:27:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/3116a097-bf1a-433e-a74a-eac8a4c281b4
17/11/01 13:27:03 INFO SessionState: Created local directory: /tmp/hive/java/root/3116a097-bf1a-433e-a74a-eac8a4c281b4
17/11/01 13:27:03 INFO SessionState: Created HDFS directory: /tmp/hive/root/3116a097-bf1a-433e-a74a-eac8a4c281b4/_tmp_space.db
17/11/01 13:27:03 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:27:04 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
17/11/01 13:27:04 INFO HiveUtils: Initializing execution hive, version 1.2.1
17/11/01 13:27:04 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:04 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:27:04 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:27:04 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/11/01 13:27:04 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/11/01 13:27:05 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:06 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/11/01 13:27:07 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:07 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:07 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:07 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:07 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:07 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:07 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/11/01 13:27:08 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/11/01 13:27:08 INFO HiveMetaStore: Added admin role in metastore
17/11/01 13:27:08 INFO HiveMetaStore: Added public role in metastore
17/11/01 13:27:08 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/11/01 13:27:08 INFO HiveMetaStore: 0: get_all_databases
17/11/01 13:27:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/11/01 13:27:08 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/11/01 13:27:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/11/01 13:27:08 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/11/01 13:27:08 INFO SessionState: Created local directory: /tmp/hive/java/ec262035-5a2a-4a05-841e-1163937e8753_resources
17/11/01 13:27:08 INFO SessionState: Created HDFS directory: /tmp/hive/root/ec262035-5a2a-4a05-841e-1163937e8753
17/11/01 13:27:08 INFO SessionState: Created local directory: /tmp/hive/java/root/ec262035-5a2a-4a05-841e-1163937e8753
17/11/01 13:27:08 INFO SessionState: Created HDFS directory: /tmp/hive/root/ec262035-5a2a-4a05-841e-1163937e8753/_tmp_space.db
17/11/01 13:27:08 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:27:08 INFO SessionManager: Operation log root directory is created: /tmp/hive/java/root/operation_logs
17/11/01 13:27:08 INFO AbstractService: HiveServer2: Async execution pool size 100
17/11/01 13:27:08 INFO AbstractService: Service:OperationManager is inited.
17/11/01 13:27:08 INFO AbstractService: Service: SessionManager is inited.
17/11/01 13:27:08 INFO AbstractService: Service: CLIService is inited.
17/11/01 13:27:08 INFO AbstractService: Service:ThriftBinaryCLIService is inited.
17/11/01 13:27:08 INFO AbstractService: Service: HiveServer2 is inited.
17/11/01 13:27:08 INFO AbstractService: Service:OperationManager is started.
17/11/01 13:27:08 INFO AbstractService: Service:SessionManager is started.
17/11/01 13:27:08 INFO AbstractService: Service:CLIService is started.
17/11/01 13:27:08 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:27:08 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:27:08 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:08 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:08 INFO HiveMetaStore: 0: get_databases: default
17/11/01 13:27:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
17/11/01 13:27:08 INFO HiveMetaStore: 0: Shutting down the object store...
17/11/01 13:27:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
17/11/01 13:27:08 INFO HiveMetaStore: 0: Metastore shutdown complete.
17/11/01 13:27:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
17/11/01 13:27:08 INFO AbstractService: Service:ThriftBinaryCLIService is started.
17/11/01 13:27:08 INFO AbstractService: Service:HiveServer2 is started.
17/11/01 13:27:08 INFO HiveThriftServer2: HiveThriftServer2 started
17/11/01 13:27:08 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
17/11/01 13:27:12 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 13:27:12 INFO SessionState: Created local directory: /tmp/hive/java/530fec0e-6949-4712-8777-52f5f8ca4584_resources
17/11/01 13:27:12 INFO SessionState: Created HDFS directory: /tmp/hive/root/530fec0e-6949-4712-8777-52f5f8ca4584
17/11/01 13:27:12 INFO SessionState: Created local directory: /tmp/hive/java/root/530fec0e-6949-4712-8777-52f5f8ca4584
17/11/01 13:27:12 INFO SessionState: Created HDFS directory: /tmp/hive/root/530fec0e-6949-4712-8777-52f5f8ca4584/_tmp_space.db
17/11/01 13:27:12 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/530fec0e-6949-4712-8777-52f5f8ca4584
17/11/01 13:27:13 INFO HiveMetaStore: 1: get_database: global_temp
17/11/01 13:27:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/11/01 13:27:13 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:27:13 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:27:13 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:27:13 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:13 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:13 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/11/01 13:27:13 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:13 INFO SessionState: Created local directory: /tmp/hive/java/c140713b-e770-40ad-ad96-41b4688579e2_resources
17/11/01 13:27:13 INFO SessionState: Created HDFS directory: /tmp/hive/root/c140713b-e770-40ad-ad96-41b4688579e2
17/11/01 13:27:13 INFO SessionState: Created local directory: /tmp/hive/java/root/c140713b-e770-40ad-ad96-41b4688579e2
17/11/01 13:27:13 INFO SessionState: Created HDFS directory: /tmp/hive/root/c140713b-e770-40ad-ad96-41b4688579e2/_tmp_space.db
17/11/01 13:27:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:27:15 INFO HiveMetaStore: 1: get_database: default
17/11/01 13:27:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:33 INFO SparkExecuteStatementOperation: Running query 'insert overwrite table default.test_e partition(pt="1") select count(1) from default.test_f' with 05657fe9-8ff4-4263-8a69-b5a3f5013d78
17/11/01 13:27:33 INFO HiveMetaStore: 2: get_database: default
17/11/01 13:27:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:33 INFO HiveMetaStore: 2: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:27:33 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:27:33 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:27:33 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:33 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:33 INFO HiveMetaStore: 2: get_database: default
17/11/01 13:27:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:33 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 13:27:33 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:34 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 13:27:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:34 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:34 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-34_860_4280324884059974583-1
17/11/01 13:27:35 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:35 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:35 INFO FileSourceStrategy: Pruning directories with: 
17/11/01 13:27:35 INFO FileSourceStrategy: Post-Scan Filters: 
17/11/01 13:27:35 INFO FileSourceStrategy: Output Data Schema: struct<>
17/11/01 13:27:35 INFO FileSourceScanExec: Pushed Filters: 
17/11/01 13:27:35 INFO HiveMetaStore: 2: get_database: default
17/11/01 13:27:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:35 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 13:27:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:35 INFO HiveMetaStore: 2: get_table : db=default tbl=test_f
17/11/01 13:27:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:35 INFO HiveMetaStore: 2: get_partitions : db=default tbl=test_f
17/11/01 13:27:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test_f	
17/11/01 13:27:35 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:27:36 INFO CodeGenerator: Code generated in 264.55479 ms
17/11/01 13:27:36 INFO CodeGenerator: Code generated in 22.828938 ms
17/11/01 13:27:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 398.4 KB, free 365.9 MB)
17/11/01 13:27:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 40.1 KB, free 365.9 MB)
17/11/01 13:27:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on node3:41106 (size: 40.1 KB, free: 366.3 MB)
17/11/01 13:27:36 INFO SparkContext: Created broadcast 0 from run at AccessController.java:0
17/11/01 13:27:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/11/01 13:27:36 INFO SparkContext: Starting job: run at AccessController.java:0
17/11/01 13:27:36 INFO DAGScheduler: Registering RDD 2 (run at AccessController.java:0)
17/11/01 13:27:36 INFO DAGScheduler: Got job 0 (run at AccessController.java:0) with 1 output partitions
17/11/01 13:27:36 INFO DAGScheduler: Final stage: ResultStage 1 (run at AccessController.java:0)
17/11/01 13:27:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/11/01 13:27:36 INFO DAGScheduler: Missing parents: List()
17/11/01 13:27:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at run at AccessController.java:0), which has no missing parents
17/11/01 13:27:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 377.4 KB, free 365.5 MB)
17/11/01 13:27:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 126.1 KB, free 365.4 MB)
17/11/01 13:27:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on node3:41106 (size: 126.1 KB, free: 366.1 MB)
17/11/01 13:27:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1029
17/11/01 13:27:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
17/11/01 13:27:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/11/01 13:27:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7754 bytes)
17/11/01 13:27:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
17/11/01 13:27:37 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/11/01 13:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
17/11/01 13:27:37 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@76aeb01d
17/11/01 13:27:37 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/11/01 13:27:37 INFO ParquetRecordWriterWrapper: initialize serde with table properties.
17/11/01 13:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-34_860_4280324884059974583-1/-ext-10000/_temporary/0/_temporary/attempt_20171101132737_0001_m_000000_0/part-00000-4fd082ba-ea5a-48d7-8461-0d8fd0e5cfa4-c000
17/11/01 13:27:37 INFO CodecConfig: Compression set to false
17/11/01 13:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
17/11/01 13:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
17/11/01 13:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
17/11/01 13:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/11/01 13:27:37 INFO ParquetOutputFormat: Dictionary is on
17/11/01 13:27:37 INFO ParquetOutputFormat: Validation is off
17/11/01 13:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/11/01 13:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@578b561c
17/11/01 13:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
17/11/01 13:27:38 INFO ColumnChunkPageWriteStore: written 38B for [name] BINARY: 1 values, 11B raw, 11B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/11/01 13:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_20171101132737_0001_m_000000_0' to file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-34_860_4280324884059974583-1/-ext-10000/_temporary/0/task_20171101132737_0001_m_000000
17/11/01 13:27:38 INFO SparkHadoopMapRedUtil: attempt_20171101132737_0001_m_000000_0: Committed
17/11/01 13:27:38 INFO ContextCleaner: Cleaned accumulator 8
17/11/01 13:27:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2526 bytes result sent to driver
17/11/01 13:27:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1302 ms on localhost (executor driver) (1/1)
17/11/01 13:27:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/11/01 13:27:38 INFO DAGScheduler: ResultStage 1 (run at AccessController.java:0) finished in 1.796 s
17/11/01 13:27:38 INFO DAGScheduler: Job 0 finished: run at AccessController.java:0, took 1.877300 s
17/11/01 13:27:38 INFO FileFormatWriter: Job null committed.
17/11/01 13:27:38 INFO FileFormatWriter: Finished processing stats for job null.
17/11/01 13:27:38 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:38 INFO HiveMetaStore: 2: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:27:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:27:38 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:38 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:38 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:38 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:39 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:39 INFO HiveMetaStore: 2: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:27:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:27:39 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/pt=1
17/11/01 13:27:39 INFO Hive: Renaming src: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-34_860_4280324884059974583-1/-ext-10000/part-00000-4fd082ba-ea5a-48d7-8461-0d8fd0e5cfa4-c000, dest: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/pt=1/part-00000-4fd082ba-ea5a-48d7-8461-0d8fd0e5cfa4-c000, Status:true
17/11/01 13:27:39 INFO HiveMetaStore: 2: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:27:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:27:39 INFO HiveMetaStore: 2: alter_partition : db=default tbl=test_e
17/11/01 13:27:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_partition : db=default tbl=test_e	
17/11/01 13:27:39 INFO HiveMetaStore: New partition values:[1]
17/11/01 13:27:39 WARN log: Updating partition stats fast for: test_e
17/11/01 13:27:39 WARN log: Updated size to 209
17/11/01 13:27:39 INFO HiveMetaStore: 2: get_table : db=default tbl=test_e
17/11/01 13:27:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:39 INFO CodeGenerator: Code generated in 20.649099 ms
17/11/01 13:27:39 INFO DAGScheduler: Asked to cancel job group 05657fe9-8ff4-4263-8a69-b5a3f5013d78
17/11/01 13:27:46 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8
17/11/01 13:27:46 INFO SessionState: Created local directory: /tmp/hive/java/1eab2141-caa1-4468-a388-5c2709e271e0_resources
17/11/01 13:27:46 INFO SessionState: Created HDFS directory: /tmp/hive/root/1eab2141-caa1-4468-a388-5c2709e271e0
17/11/01 13:27:46 INFO SessionState: Created local directory: /tmp/hive/java/root/1eab2141-caa1-4468-a388-5c2709e271e0
17/11/01 13:27:46 INFO SessionState: Created HDFS directory: /tmp/hive/root/1eab2141-caa1-4468-a388-5c2709e271e0/_tmp_space.db
17/11/01 13:27:46 INFO HiveSessionImpl: Operation log session directory is created: /tmp/hive/java/root/operation_logs/1eab2141-caa1-4468-a388-5c2709e271e0
17/11/01 13:27:46 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:46 INFO SessionState: Created local directory: /tmp/hive/java/caab23ca-9d79-4c77-bc00-16d9305fcc2e_resources
17/11/01 13:27:46 INFO SessionState: Created HDFS directory: /tmp/hive/root/caab23ca-9d79-4c77-bc00-16d9305fcc2e
17/11/01 13:27:46 INFO SessionState: Created local directory: /tmp/hive/java/root/caab23ca-9d79-4c77-bc00-16d9305fcc2e
17/11/01 13:27:46 INFO SessionState: Created HDFS directory: /tmp/hive/root/caab23ca-9d79-4c77-bc00-16d9305fcc2e/_tmp_space.db
17/11/01 13:27:46 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
17/11/01 13:27:46 INFO HiveMetaStore: 3: get_database: default
17/11/01 13:27:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:46 INFO HiveMetaStore: 3: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:27:46 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:27:46 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:27:46 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:46 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:48 INFO SparkExecuteStatementOperation: Running query 'insert overwrite table default.test_e partition(pt="1") select count(1) from default.test_f' with b07a1523-d186-4dbf-9c1e-559ede48a379
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_database: default
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:48 INFO HiveMetaStore: 4: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/11/01 13:27:48 INFO ObjectStore: ObjectStore, initialize called
17/11/01 13:27:48 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
17/11/01 13:27:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/11/01 13:27:48 INFO ObjectStore: Initialized ObjectStore
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_database: default
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:48 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-48_224_1938618776392737864-2
17/11/01 13:27:48 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:48 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:48 INFO FileSourceStrategy: Pruning directories with: 
17/11/01 13:27:48 INFO FileSourceStrategy: Post-Scan Filters: 
17/11/01 13:27:48 INFO FileSourceStrategy: Output Data Schema: struct<>
17/11/01 13:27:48 INFO FileSourceScanExec: Pushed Filters: 
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_database: default
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_table : db=default tbl=test_f
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_f	
17/11/01 13:27:48 INFO HiveMetaStore: 4: get_partitions : db=default tbl=test_f
17/11/01 13:27:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test_f	
17/11/01 13:27:48 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:27:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 398.4 KB, free 365.0 MB)
17/11/01 13:27:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 40.1 KB, free 365.0 MB)
17/11/01 13:27:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on node3:41106 (size: 40.1 KB, free: 366.1 MB)
17/11/01 13:27:48 INFO SparkContext: Created broadcast 2 from run at AccessController.java:0
17/11/01 13:27:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/11/01 13:27:48 INFO SparkContext: Starting job: run at AccessController.java:0
17/11/01 13:27:48 INFO DAGScheduler: Registering RDD 7 (run at AccessController.java:0)
17/11/01 13:27:48 INFO DAGScheduler: Got job 1 (run at AccessController.java:0) with 1 output partitions
17/11/01 13:27:48 INFO DAGScheduler: Final stage: ResultStage 3 (run at AccessController.java:0)
17/11/01 13:27:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/11/01 13:27:48 INFO DAGScheduler: Missing parents: List()
17/11/01 13:27:48 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0), which has no missing parents
17/11/01 13:27:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 377.4 KB, free 364.6 MB)
17/11/01 13:27:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 126.1 KB, free 364.5 MB)
17/11/01 13:27:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on node3:41106 (size: 126.1 KB, free: 366.0 MB)
17/11/01 13:27:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1029
17/11/01 13:27:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
17/11/01 13:27:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/11/01 13:27:49 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7754 bytes)
17/11/01 13:27:49 INFO Executor: Running task 0.0 in stage 3.0 (TID 1)
17/11/01 13:27:49 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/11/01 13:27:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/11/01 13:27:49 WARN JobConf: The variable mapred.child.ulimit is no longer used.
17/11/01 13:27:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/11/01 13:27:49 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c675df2
17/11/01 13:27:49 INFO ParquetRecordWriterWrapper: initialize serde with table properties.
17/11/01 13:27:49 INFO ParquetRecordWriterWrapper: creating real writer to write at file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-48_224_1938618776392737864-2/-ext-10000/_temporary/0/_temporary/attempt_20171101132749_0003_m_000000_0/part-00000-0286576d-a6ac-4855-bb44-90f17434f75d-c000
17/11/01 13:27:49 INFO CodecConfig: Compression set to false
17/11/01 13:27:49 INFO CodecConfig: Compression: UNCOMPRESSED
17/11/01 13:27:49 INFO ParquetOutputFormat: Parquet block size to 134217728
17/11/01 13:27:49 INFO ParquetOutputFormat: Parquet page size to 1048576
17/11/01 13:27:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/11/01 13:27:49 INFO ParquetOutputFormat: Dictionary is on
17/11/01 13:27:49 INFO ParquetOutputFormat: Validation is off
17/11/01 13:27:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/11/01 13:27:49 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@e16617
17/11/01 13:27:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9
17/11/01 13:27:49 INFO ColumnChunkPageWriteStore: written 38B for [name] BINARY: 1 values, 11B raw, 11B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/11/01 13:27:49 INFO FileOutputCommitter: Saved output of task 'attempt_20171101132749_0003_m_000000_0' to file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-48_224_1938618776392737864-2/-ext-10000/_temporary/0/task_20171101132749_0003_m_000000
17/11/01 13:27:49 INFO SparkHadoopMapRedUtil: attempt_20171101132749_0003_m_000000_0: Committed
17/11/01 13:27:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 2483 bytes result sent to driver
17/11/01 13:27:49 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 1) in 114 ms on localhost (executor driver) (1/1)
17/11/01 13:27:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/11/01 13:27:49 INFO DAGScheduler: ResultStage 3 (run at AccessController.java:0) finished in 0.457 s
17/11/01 13:27:49 INFO DAGScheduler: Job 1 finished: run at AccessController.java:0, took 0.464293 s
17/11/01 13:27:49 INFO FileFormatWriter: Job null committed.
17/11/01 13:27:49 INFO FileFormatWriter: Finished processing stats for job null.
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:27:49 INFO FileUtils: Creating directory if it doesn't exist: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/pt=1
17/11/01 13:27:49 INFO Hive: Renaming src: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/.hive-staging_hive_2017-11-01_13-27-48_224_1938618776392737864-2/-ext-10000/part-00000-0286576d-a6ac-4855-bb44-90f17434f75d-c000, dest: file:/opt/spark/spark_source_code/spark/spark-warehouse/test_e/pt=1/part-00000-0286576d-a6ac-4855-bb44-90f17434f75d-c000, Status:true
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_partition_with_auth : db=default tbl=test_e[1]
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test_e[1]	
17/11/01 13:27:49 INFO HiveMetaStore: 4: alter_partition : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=alter_partition : db=default tbl=test_e	
17/11/01 13:27:49 INFO HiveMetaStore: New partition values:[1]
17/11/01 13:27:49 WARN log: Updating partition stats fast for: test_e
17/11/01 13:27:49 WARN log: Updated size to 209
17/11/01 13:27:49 INFO HiveMetaStore: 4: get_table : db=default tbl=test_e
17/11/01 13:27:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test_e	
17/11/01 13:27:49 INFO DAGScheduler: Asked to cancel job group b07a1523-d186-4dbf-9c1e-559ede48a379
17/11/01 13:28:29 ERROR HiveThriftServer2: RECEIVED SIGNAL TERM
17/11/01 13:28:29 INFO HiveServer2: Shutting down HiveServer2
17/11/01 13:28:29 INFO ThriftCLIService: Thrift server has stopped
17/11/01 13:28:29 INFO AbstractService: Service:ThriftBinaryCLIService is stopped.
17/11/01 13:28:29 INFO AbstractService: Service:OperationManager is stopped.
17/11/01 13:28:29 INFO AbstractService: Service:SessionManager is stopped.
17/11/01 13:28:29 INFO AbstractService: Service:CLIService is stopped.
17/11/01 13:28:29 INFO AbstractService: Service:HiveServer2 is stopped.
17/11/01 13:28:29 INFO SparkUI: Stopped Spark web UI at http://node3:4040
17/11/01 13:28:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/11/01 13:28:29 INFO MemoryStore: MemoryStore cleared
17/11/01 13:28:29 INFO BlockManager: BlockManager stopped
17/11/01 13:28:29 INFO BlockManagerMaster: BlockManagerMaster stopped
17/11/01 13:28:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/11/01 13:28:29 INFO SparkContext: Successfully stopped SparkContext
17/11/01 13:28:29 INFO ShutdownHookManager: Shutdown hook called
17/11/01 13:28:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-c4000990-2b7e-479f-a27c-bb469433a6bc
17/11/01 13:28:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc6ec9f9-8233-493d-bd0a-1d6e6be3adbb
17/11/01 13:28:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d7a372d-fb22-47f9-90a9-daa67a2a6912
